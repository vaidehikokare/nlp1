{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5246dc39-6f96-4836-a104-d5d4a9cac4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kokar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kokar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f7526dc-ebec-4b66-904d-b8fc840107ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace tokenization\n",
      "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'Natural', 'Language', 'Processing!', \"Let's\", 'tokenize', 'and', 'stem.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,regexp_tokenize,TweetTokenizer,MWETokenizer ,TreebankWordTokenizer\n",
    "sentence = \"NLTK is a powerful library for Natural Language Processing! Let's tokenize and stem.\"\n",
    "# Whitespace tokenization splits the sentence based on spaces (whitespace characters).\n",
    "# It’s a simple technique where each word or sequence of characters separated by whitespace is treated as a separate token.\n",
    "# Here's how the output will look for whitespace tokenization using the sample sentence:\n",
    "print(\"Whitespace tokenization\")\n",
    "Whitespace_tokenization=sentence.split()\n",
    "print(Whitespace_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f855be2f-a10c-4675-9a44-f6f583463871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation-based tokenization\n",
      "[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '!', ' ', ' ', ' ', ' ', '.']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation-based tokenization splits tokens based on whitespace and punctuation marks\n",
    "# (e.g., commas, periods, exclamation marks, etc.). In this approach, punctuation is treated as separate tokens. \n",
    "# Here’s how the output looks when we use a regular expression to tokenize the sentence by whitespace or punctuation\n",
    "print(\"Punctuation-based tokenization\")\n",
    "toke=regexp_tokenize(sentence,pattern=r'\\s|[\\.,?!;\"]')\n",
    "print(toke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3c2a23e-2075-4b4a-b816-3fbada2b57eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank Tokenization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'library',\n",
       " 'for',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '!',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'tokenize',\n",
       " 'and',\n",
       " 'stem',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Treebank Tokenization is a method of tokenizing text based on rules that are specifically \n",
    "# designed for processing English text as it appears in the Penn Treebank corpus. \n",
    "# It is part of the nltk.tokenize module and handles more complex tokenization cases, such as contractions, \n",
    "# possessive forms, punctuation, and special characters, with greater accuracy than simpler methods \n",
    "# like whitespace-based or punctuation-based tokenization.\n",
    "\n",
    "# Key Features:\n",
    "# Handles contractions: It splits contractions (e.g., \"don't\" into \"do\" and \"n't\").\n",
    "\n",
    "# Handles possessives: It separates possessive forms (e.g., \"John's\" becomes \"John\" and \"'s\").\n",
    "\n",
    "# Accurate punctuation handling: It separates punctuation like periods and \n",
    "# commas as individual tokens, while still associating them with the words they follow.\n",
    "print(\"Treebank Tokenization\")\n",
    "tokenation=TreebankWordTokenizer()\n",
    "tokenation.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "316f81a3-ba85-431b-b439-c0fbf3ee9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tweet Tokenization:\n",
      "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'Natural', 'Language', 'Processing', '!', \"Let's\", 'tokenize', 'and', 'stem', '.']\n"
     ]
    }
   ],
   "source": [
    "# weet Tokenization is a method of tokenizing text specifically designed to \n",
    "# handle the unique characteristics of social media text, particularly tweets. \n",
    "# Tweets often contain hashtags, mentions, emoticons, and informal language, making them different from regular text.\n",
    "# The TweetTokenizer in the nltk.tokenize module is tailored to handle these elements properly.\n",
    "\n",
    "# Features of Tweet Tokenization:\n",
    "# Handles Hashtags: It keeps hashtags intact as a single token (e.g., #NLTK stays as #NLTK).\n",
    "\n",
    "# Handles Mentions: Mentions (e.g., @user) are treated as individual tokens.\n",
    "\n",
    "# Handles Emojis and Emoticons: It keeps emoticons and emojis as separate tokens.\n",
    "\n",
    "# Handles URLs: URLs are separated as tokens (e.g., http://example.com becomes a single token).\n",
    "\n",
    "# Handles Retweets: \"RT\" is treated as a token in retweets.\n",
    "print(\"\\nTweet Tokenization:\")\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "tweet_tokens = tweet_tokenizer.tokenize(sentence)\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a53dbe7b-4fff-4ba5-8ade-a11ae7514fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Multi-Word Expression) Tokenization \n",
      "['NLTK', 'is', 'a', 'powerful', 'library', 'for', 'Natural_Language', 'Processing!', \"Let's\", 'tokenize', 'and', 'stem.']\n"
     ]
    }
   ],
   "source": [
    "# MWE (Multi-Word Expression) Tokenization refers to tokenizing text in such a way that multi-word expressions (MWEs) \n",
    "# are treated as single tokens. MWEs are sequences of words that form a single unit of meaning, such as:\n",
    "\n",
    "# Phrasal verbs: \"take off\", \"give up\"\n",
    "\n",
    "# Idiomatic expressions: \"kick the bucket\", \"break the ice\"\n",
    "\n",
    "# Named entities: \"New York\", \"United States\"\n",
    "\n",
    "# Collocations: \"strong coffee\", \"fast food\"\n",
    "print(\"(Multi-Word Expression) Tokenization \")\n",
    "tokenazation=MWETokenizer([(\"Natural\",\"Language\")])\n",
    "a=tokenazation.tokenize(sentence.split())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ec3a11-a6c1-467d-8246-d72171163807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm: The Snowball Stemmer, also known as the English Snowball Stemmer, is an improved version of the Porter Stemmer. It follows a more flexible, generalized approach and has been expanded to cover more languages.\n",
    "\n",
    "# Language Coverage: The Snowball Stemmer supports multiple languages, including English, French, German, Italian, Dutch, and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2669aad-9751-476a-905a-3b81834ce8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer , SnowballStemmer ,WordNetLemmatizer\n",
    "porter_stemmer = PorterStemmer()\n",
    "snowball_stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7ae5013e-37e0-4d9e-b4f8-f7b185dc2e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "poter_steam=[porter_stemmer.stem(word) for word in word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe749a18-d430-454c-8e1a-8aef79336199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nltk',\n",
       " 'is',\n",
       " 'a',\n",
       " 'power',\n",
       " 'librari',\n",
       " 'for',\n",
       " 'natur',\n",
       " 'languag',\n",
       " 'process',\n",
       " '!',\n",
       " 'let',\n",
       " \"'s\",\n",
       " 'token',\n",
       " 'and',\n",
       " 'stem',\n",
       " '.']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poter_steam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44e5af71-1311-48b7-85ca-e4f9b844fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitation=WordNetLemmatizer()\n",
    "lemmatized_words = [lemmitation.lemmatize(word) for word in word_tokenize(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "430bcd72-0575-412a-a788-f2f9c507bfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'powerful',\n",
       " 'library',\n",
       " 'for',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '!',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'tokenize',\n",
       " 'and',\n",
       " 'stem',\n",
       " '.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dddc528-2b3f-4d62-9703-c551d86a2d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
